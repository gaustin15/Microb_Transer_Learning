{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_functions import tune_SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_abundance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-27 15:57:40] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n",
      "[INFO 03-27 15:57:40] ax.service.managed_loop: Started full optimization with 8 steps.\n",
      "[INFO 03-27 15:57:40] ax.service.managed_loop: Running optimization trial 1...\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "Checkpoint directory checkpoint_dir exists and is not empty.\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/nn/modules/container.py:117: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "[INFO 03-27 15:57:44] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Started full optimization with 10 steps.\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 03-27 15:57:44] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 03-27 15:57:45] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 03-27 15:57:45] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 03-27 15:57:45] ax.service.managed_loop: Running optimization trial 2...\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "Checkpoint directory checkpoint_dir exists and is not empty.\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/nn/modules/container.py:117: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "[INFO 03-27 15:57:46] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n",
      "[INFO 03-27 15:57:46] ax.service.managed_loop: Started full optimization with 10 steps.\n",
      "[INFO 03-27 15:57:46] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 03-27 15:57:49] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 03-27 15:57:50] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 03-27 15:57:55] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 03-27 15:58:02] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 03-27 15:58:08] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 03-27 15:58:15] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 03-27 15:58:21] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 03-27 15:58:23] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 03-27 15:58:29] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 03-27 15:58:37] ax.service.managed_loop: Running optimization trial 3...\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "Checkpoint directory checkpoint_dir exists and is not empty.\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/nn/modules/container.py:117: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "/Users/george/anaconda2/envs/dnabert/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning:\n",
      "\n",
      "The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_results = tune_SAE(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "dnabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
