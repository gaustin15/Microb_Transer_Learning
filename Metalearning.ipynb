{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need older version of lightning for this notebook to run\n",
    "\n",
    "import sys\n",
    "sys.path.append('learn2learn/') # the version on pypi doesn't have LightningMAML\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "import learn2learn as l2l\n",
    "from learn2learn.data import TaskDataset\n",
    "from learn2learn.algorithms.lightning import LightningMAML\n",
    "from learn2learn.utils.lightning import EpisodicBatcher\n",
    "\n",
    "from baseline.pytorch_models import LitFFNN\n",
    "from baseline.training_functions import *\n",
    "from baseline.training_functions import make_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datasets import load_abundance_data, get_shared_taxa_dfs\n",
    "from datasets import MicroDataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "dfs = load_abundance_data()\n",
    "all_datasets = get_shared_taxa_dfs(dfs)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>Quin_gut_liver_cirrhosis</td>\n",
       "      <td>[1024, 128, 128, 0.1, 0.5]</td>\n",
       "      <td>0.746377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>WT2D</td>\n",
       "      <td>[128, 64, 32, 0.001, 0.5]</td>\n",
       "      <td>0.595960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>Zeller_fecal_colorectal_cancer</td>\n",
       "      <td>[256, 256, 32, 0.1, 0.1]</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>Chatelier_gut_obesity</td>\n",
       "      <td>[1024, 128, 32, 0.01, 0.1]</td>\n",
       "      <td>0.690311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>metahit</td>\n",
       "      <td>[1024, 64, 128, 0.1, 0.5]</td>\n",
       "      <td>0.623529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Abundance</td>\n",
       "      <td>t2dmeta_long</td>\n",
       "      <td>[256, 256, 128, 0.1, 0.1]</td>\n",
       "      <td>0.605882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type  Data Type                         Dataset  \\\n",
       "0       FFNN  Abundance        Quin_gut_liver_cirrhosis   \n",
       "1       FFNN  Abundance                            WT2D   \n",
       "2       FFNN  Abundance  Zeller_fecal_colorectal_cancer   \n",
       "3       FFNN  Abundance           Chatelier_gut_obesity   \n",
       "4       FFNN  Abundance                         metahit   \n",
       "5       FFNN  Abundance                    t2dmeta_long   \n",
       "\n",
       "                  hyperparams       AUC  \n",
       "0  [1024, 128, 128, 0.1, 0.5]  0.746377  \n",
       "1   [128, 64, 32, 0.001, 0.5]  0.595960  \n",
       "2    [256, 256, 32, 0.1, 0.1]  0.700000  \n",
       "3  [1024, 128, 32, 0.01, 0.1]  0.690311  \n",
       "4   [1024, 64, 128, 0.1, 0.5]  0.623529  \n",
       "5   [256, 256, 128, 0.1, 0.1]  0.605882  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('results/baseline/ffnn_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [b for a,b in all_datasets.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [make_split(df) for df in datasets]\n",
    "\n",
    "#redo the split for the dataset of interest (the one at idx 0)\n",
    "# so we include a test set\n",
    "train, test = make_split(datasets[0])\n",
    "train, valid = make_split(train)\n",
    "\n",
    "splits[0] = (train, valid)\n",
    "\n",
    "trains = [s[0] for s in splits]\n",
    "vals = [s[1] for s in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaMicroDataset(Dataset):\n",
    "    \"\"\"Dataset class for column dataset.\n",
    "    Args:\n",
    "       cats (list of str): List of the name of columns contain\n",
    "                           categorical variables.\n",
    "       conts (list of str): List of the name of columns which \n",
    "                           contain continuous variables.\n",
    "       y (Tensor, optional): Target variables.\n",
    "       is_reg (bool): If the task is regression, set ``True``, \n",
    "                      otherwise (classification) ``False``.\n",
    "       is_multi (bool): If the task is multi-label classification, \n",
    "                        set ``True``.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, is_marker = False):\n",
    "        df = df.sample(frac=1)\n",
    "        if not is_marker:\n",
    "            self.taxa_cols = df.columns[df.columns.str.contains('k__')] \n",
    "        else:\n",
    "            self.taxa_cols = df.columns[df.columns.str.contains('gi[|]')] \n",
    "        self.matrix = torch.Tensor( df[self.taxa_cols].astype(float).values ).float()\n",
    "        \n",
    "        #scale the dataset to be in relative abundance space\n",
    "        self.matrix=F.softmax(self.matrix, requires_grad=True)\n",
    "        \n",
    "        df.loc[df.disease=='ibd_crohn_disease'] = 'ibd_ulcerative_colitis'\n",
    "        self.y=torch.Tensor( pd.Categorical(df.disease).codes ).long()\n",
    "        self.n_samples, self.n_taxa= self.matrix.shape\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.matrix[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(a):\n",
    "    \"\"\"\n",
    "    collate function to simplify the tasks -- only want to ever distinguish between class 0 and 1\n",
    "    Each class can represent any positive/negative group from any dataset\n",
    "    6 datasets ==> 12 classes ==> 132 distinct metalearning tasks\n",
    "    \"\"\"\n",
    "    idx = max([b[1] for b in a])\n",
    "    #print(a[1])\n",
    "    return(torch.cat( [b[0].unsqueeze(0) for b in a] ),\n",
    "           torch.Tensor( [int(b[1]==idx)  for b in a]) )#.long() )\n",
    "\n",
    "def collate(a):\n",
    "    \"\"\"\n",
    "    collate function to simplify the tasks -- only want to ever distinguish between class 0 and 1\n",
    "    Each class can represent any positive/negative group from any dataset\n",
    "    6 datasets ==> 12 classes ==> 132 distinct metalearning tasks\n",
    "    \"\"\"\n",
    "    #idx = max([b[1] for b in a])\n",
    "    q = torch.Tensor([b[1] for b in a])\n",
    "    return(torch.cat( [b[0].unsqueeze(0) for b in a] ),\n",
    "           ( q == q.max() ).long() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_taskset(datasets):\n",
    "    MetaDS = l2l.data.UnionMetaDataset( [l2l.data.MetaDataset( MicroDataset(t) ) for t in datasets] )\n",
    "    dataset = l2l.data.MetaDataset(MetaDS)\n",
    "    transforms = [\n",
    "        l2l.data.transforms.NWays(dataset, n=2),\n",
    "        l2l.data.transforms.KShots(dataset, k=8),\n",
    "        l2l.data.transforms.LoadData(dataset)\n",
    "    ]\n",
    "    return( TaskDataset(dataset, transforms,\n",
    "                        num_tasks=2*( len(datasets) * 2 * (len(datasets) * 2 - 1) ), \n",
    "                        task_collate=collate)\n",
    "          )\n",
    "    \n",
    "train_set = build_taskset(trains)\n",
    "val_set = build_taskset(vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = build_taskset(trains)\n",
    "val_set = build_taskset(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # MetaDS = MetaMicroDataset(trains)\n",
    "# MetaDS = l2l.data.UnionMetaDataset( [l2l.data.MetaDataset( MicroDataset(t) ) for t in datasets] )\n",
    "# dataset = l2l.data.MetaDataset(MetaDS)\n",
    "# transforms = [\n",
    "#     l2l.data.transforms.NWays(dataset, n=2),\n",
    "#     l2l.data.transforms.KShots(dataset, k=10),\n",
    "#     l2l.data.transforms.LoadData(dataset)\n",
    "# ]\n",
    "# train_set = TaskDataset(dataset, transforms,\n",
    "#                         num_tasks=( len(datasets) * 2 * (len(datasets) * 2 - 1) ), \n",
    "#                         task_collate=collate)\n",
    "# for task in taskset:\n",
    "#     X, y = task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    a deep FFNN network -- assuming it's going to show improvements\n",
    "    in the transfer learning exploration\n",
    "    probably won't be so good in standard learning approach\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 layer_sizes = [128, 64, 32], \n",
    "                 dropout = .2, \n",
    "                 n_labels = 2):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        linear_layers = [ nn.Linear( dataset.n_taxa, layer_sizes[0]), \n",
    "                          nn.BatchNorm1d(layer_sizes[0]), \n",
    "                          nn.Dropout(dropout), \n",
    "                          nn.GELU()]\n",
    "        self.n_labels=n_labels\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            linear_layers += [ nn.Linear(layer_sizes[i], layer_sizes[i+1]), \n",
    "                               nn.BatchNorm1d(layer_sizes[i+1]), \n",
    "                               nn.Dropout(dropout), \n",
    "                               nn.GELU()]\n",
    "            \n",
    "        linear_layers += [ nn.Linear(layer_sizes[-1], n_labels), \n",
    "                           nn.Softmax() ]\n",
    "        \n",
    "        self.linear_net = nn.Sequential(*linear_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_net(x.float())\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(2) * (maml.train_queries + maml.train_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for task in train_set:\n",
    "    X, y = task\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 771])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(maml.train_shots):\n",
    "#     qqq=np.arange(2) * (maml.train_queries + maml.train_shots) + _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_taskset(datasets):\n",
    "    MetaDS = l2l.data.UnionMetaDataset( [l2l.data.MetaDataset( MicroDataset(t) ) for t in datasets] )\n",
    "    dataset = l2l.data.MetaDataset(MetaDS)\n",
    "    transforms = [\n",
    "        l2l.data.transforms.NWays(dataset, n=2),\n",
    "        l2l.data.transforms.KShots(dataset, k=k),\n",
    "        l2l.data.transforms.LoadData(dataset)\n",
    "    ]\n",
    "    return( TaskDataset(dataset, transforms,\n",
    "                        num_tasks=2*( len(datasets) * 2 * (len(datasets) * 2 - 1) ), \n",
    "                        task_collate=collate)\n",
    "          )\n",
    "    \n",
    "train_set = build_taskset(trains)\n",
    "val_set = build_taskset(vals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                       | Type             | Params\n",
      "-----------------------------------------------------------------\n",
      "0  | loss                       | CrossEntropyLoss | 0     \n",
      "1  | model                      | MAML             | 109 K \n",
      "2  | model.module               | FFNN             | 109 K \n",
      "3  | model.module.linear_net    | Sequential       | 109 K \n",
      "4  | model.module.linear_net.0  | Linear           | 98 K  \n",
      "5  | model.module.linear_net.1  | BatchNorm1d      | 256   \n",
      "6  | model.module.linear_net.2  | Dropout          | 0     \n",
      "7  | model.module.linear_net.3  | GELU             | 0     \n",
      "8  | model.module.linear_net.4  | Linear           | 8 K   \n",
      "9  | model.module.linear_net.5  | BatchNorm1d      | 128   \n",
      "10 | model.module.linear_net.6  | Dropout          | 0     \n",
      "11 | model.module.linear_net.7  | GELU             | 0     \n",
      "12 | model.module.linear_net.8  | Linear           | 2 K   \n",
      "13 | model.module.linear_net.9  | BatchNorm1d      | 64    \n",
      "14 | model.module.linear_net.10 | Dropout          | 0     \n",
      "15 | model.module.linear_net.11 | GELU             | 0     \n",
      "16 | model.module.linear_net.12 | Linear           | 66    \n",
      "17 | model.module.linear_net.13 | Softmax          | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1801309e851498cbf27c97adc3d0dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7db9ae74e04ad1873443a4f0073cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e36b3697b6474884777757d529cc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f623c6ceb44bba9afb20994e8723a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8ceabd2aee40f08412494d1c50f348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9480c25d5e80421d92522e34c43b5895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3709452c4ce840f2ae851a12ee95ddb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95703c7d03c74b6bb85ac3dfe188cb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f803b429e5e64863ada05b8ab0c9b977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454b0999320e450cbcd38fc1de902d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dbcc0e471f4eaeb04990e47e96143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd52ca36ba8439298b6b63e6a7fd3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0f894b47c44a16b1e10558aa54a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d97391331e48ac933c9d653c1f2547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6bc382d97a4a3e958c595a58f8872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79220c7f79b94d5cac21f96b527bef7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201b6c5396b749a6a0b95107cd190f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d013359d7a4e0bafa474200f4a0962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d779ef2660f4fc3a422d76bff2ff17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1189f4f8b88449c48b0cee6a2c286b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7590cd5574ea4cf99ad9ca1ce6fccc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629f945530f342aa8ef145a56cc0976d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123dc487fc824d07959c0742358621c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6920c395eea4f1e9779840d44946c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a4da1a622e49fa9c610b68da1ce1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf003533e914e68a3db7328a5d1eaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a68575cd87498787bedc9a14de0491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4ad1969d2a4abc9382c067248aec46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c611c8739a4bbc9e8532da96837e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d8618324642cd9a06fdd2f97a498d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b7222e588f4bcb8275447632840b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c279ccd3a54e7b9a5c61dad53b5477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "tensor([ True, False,  True, False])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "model = FFNN(dataset=MicroDataset(pd.concat(trains).reset_index() ))\n",
    "maml = LightningMAML(model, adaptation_lr=0.1, lr = .002,\n",
    "                    train_ways=2, \n",
    "                    test_ways=2,\n",
    "                    train_shots = k//2,\n",
    "                    test_shots = k//2,\n",
    "                    train_queries = k//2,\n",
    "                    test_queries = k//2,\n",
    "                    adaptation_steps=k//2)#, loss=torch.nn.MSELoss())\n",
    "# , allow_nograd=True, \n",
    "#                     allow_unused=True)\n",
    "episodic_data = EpisodicBatcher(train_set, train_set)#, taskset)\n",
    "#EpisodicBatcher(tasksets.train, tasksets.validation, tasksets.test)\n",
    "\n",
    "#setr up the trainer/logger/callbacks\n",
    "checkpoint_callback=ModelCheckpoint(\n",
    "                filepath = 'checkpoint_dir',\n",
    "                save_top_k=1,\n",
    "                verbose=False,\n",
    "                monitor='valid_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "\n",
    "tube_logger = TestTubeLogger('checkpoint_dir', \n",
    "                            name='test_tube_logger')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 30,\n",
    "#              num_sanity_val_steps=0,\n",
    "             progress_bar_refresh_rate=1,\n",
    "             weights_summary='full',\n",
    "             check_val_every_n_epoch=1,\n",
    "             checkpoint_callback=checkpoint_callback,\n",
    "            callbacks=[EarlyStopping(monitor='valid_loss', \n",
    "                                    patience=50)]) \n",
    "\n",
    "trainer.fit(maml, episodic_data)\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ea823d42f4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqqq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dnabert/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "qqq=torch.load(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load best_model_path's params\n",
    "maml.load_state_dict(torch.load(checkpoint_callback.best_model_path)['state_dict'])\n",
    "\n",
    "#set up standard model ==> for lightning training\n",
    "lightning = LitFFNN(train, \n",
    "                    valid)\n",
    "lightning.model.load_state_dict(maml.model.module.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (linear_net): Sequential(\n",
       "    (0): Linear(in_features=771, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): GELU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): GELU()\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): GELU()\n",
       "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (13): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7f5b641d12d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m lightning = LitFFNN(train, \n\u001b[1;32m      3\u001b[0m                     valid)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "checkpoint_callback.best_model_score\n",
    "lightning = LitFFNN(train, \n",
    "                    valid)\n",
    "lightning.model.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Trainer(max_epochs = 100,\n",
    "             logger=tube_logger,\n",
    "             progress_bar_refresh_rate=0,\n",
    "             weights_summary=None,\n",
    "             check_val_every_n_epoch=1,\n",
    "             checkpoint_callback=checkpoint_callback,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                    patience=20)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaDS = MetaMicroDataset(vals)\n",
    "dataset = l2l.data.MetaDataset(MetaDS)\n",
    "# transforms = [\n",
    "#     l2l.data.transforms.NWays(dataset, n=5),\n",
    "#     l2l.data.transforms.KShots(dataset, k=1),\n",
    "#     l2l.data.transforms.LoadData(dataset),\n",
    "# ]\n",
    "# val_taskset = TaskDataset(dataset, transforms, num_tasks=20)\n",
    "# for task in val_taskset:\n",
    "#     X, y = task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taskset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.training_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Example for running few-shot algorithms with the PyTorch Lightning wrappers.\n",
    "\"\"\"\n",
    "\n",
    "import learn2learn as l2l\n",
    "import pytorch_lightning as pl\n",
    "from argparse import ArgumentParser\n",
    "from learn2learn.algorithms import (\n",
    "    LightningPrototypicalNetworks,\n",
    "    LightningMetaOptNet,\n",
    "    LightningMAML,\n",
    "    LightningANIL,\n",
    ")\n",
    "from learn2learn.utils.lightning import EpisodicBatcher\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser(conflict_handler=\"resolve\", add_help=True)\n",
    "    # add model and trainer specific args\n",
    "    parser = LightningPrototypicalNetworks.add_model_specific_args(parser)\n",
    "    parser = LightningMetaOptNet.add_model_specific_args(parser)\n",
    "    parser = LightningMAML.add_model_specific_args(parser)\n",
    "    parser = LightningANIL.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # add script-specific args\n",
    "    parser.add_argument(\"--algorithm\", type=str, default=\"protonet\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"mini-imagenet\")\n",
    "    parser.add_argument(\"--root\", type=str, default=\"~/data\")\n",
    "    parser.add_argument(\"--meta_batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    return(parser)\n",
    "    args = parser.parse_args()\n",
    "    dict_args = vars(args)\n",
    "\n",
    "    pl.seed_everything(args.seed)\n",
    "    print('hi')\n",
    "    print(args)\n",
    "    # Create tasksets using the benchmark interface\n",
    "    if False and args.dataset in [\"mini-imagenet\", \"tiered-imagenet\"]:\n",
    "        data_augmentation = \"lee2019\"\n",
    "    else:\n",
    "        data_augmentation = \"normalize\"\n",
    "    tasksets = l2l.vision.benchmarks.get_tasksets(\n",
    "        name=args.dataset,\n",
    "        train_samples=args.train_queries + args.train_shots,\n",
    "        train_ways=args.train_ways,\n",
    "        test_samples=args.test_queries + args.test_shots,\n",
    "        test_ways=args.test_ways,\n",
    "        root=args.root,\n",
    "        data_augmentation=data_augmentation,\n",
    "    )\n",
    "    episodic_data = EpisodicBatcher(\n",
    "        tasksets.train,\n",
    "        tasksets.validation,\n",
    "        tasksets.test,\n",
    "        epoch_length=args.meta_batch_size * 10,\n",
    "    )\n",
    "\n",
    "    # init model\n",
    "    if args.dataset in [\"mini-imagenet\", \"tiered-imagenet\"]:\n",
    "        model = l2l.vision.models.ResNet12(output_size=args.train_ways)\n",
    "    else:  # CIFAR-FS, FC100\n",
    "        model = l2l.vision.models.CNN4(\n",
    "            output_size=args.train_ways,\n",
    "            hidden_size=64,\n",
    "            embedding_size=64*4,\n",
    "        )\n",
    "    features = model.features\n",
    "    classifier = model.classifier\n",
    "\n",
    "    # init algorithm\n",
    "    if args.algorithm == \"protonet\":\n",
    "        algorithm = LightningPrototypicalNetworks(features=features, **dict_args)\n",
    "    elif args.algorithm == \"maml\":\n",
    "        algorithm = LightningMAML(model, **dict_args)\n",
    "    elif args.algorithm == \"anil\":\n",
    "        algorithm = LightningANIL(features, classifier, **dict_args)\n",
    "    elif args.algorithm == \"metaoptnet\":\n",
    "        algorithm = LightningMetaOptNet(features, **dict_args)\n",
    "\n",
    "    trainer = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        gpus=1,\n",
    "        accumulate_grad_batches=args.meta_batch_size,\n",
    "        callbacks=[\n",
    "            l2l.utils.lightning.TrackTestAccuracyCallback(),\n",
    "            l2l.utils.lightning.NoLeaveProgressBar(),\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(model=algorithm, datamodule=episodic_data)\n",
    "    trainer.test(ckpt_path=\"best\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(conflict_handler=\"resolve\", add_help=True)\n",
    "# add model and trainer specific args\n",
    "# parser = LightningPrototypicalNetworks.add_model_specific_args(parser)\n",
    "# parser = LightningMetaOptNet.add_model_specific_args(parser)\n",
    "# parser = LightningMAML.add_model_specific_args(parser)\n",
    "# # parser = LightningANIL.add_model_specific_args(parser)\n",
    "# # parser = pl.Trainer.add_argparse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning = LitFFNN(trains, \n",
    "                    vals, \n",
    "                    layer_1_dim = 256,#hyperparams['layer_1_size'],\n",
    "                    layer_2_dim = 256,#, hyperparams['layer_2_size'],\n",
    "                    layer_3_dim = 32, #hyperparams['layer_3_size'],\n",
    "                    learning_rate = .1,#hyperparams['learning_rate'],\n",
    "                    batch_size=50, \n",
    "                    dropout=.2#hyperparams['dropout']\n",
    "                    )\n",
    "\n",
    "#setr up the trainer/logger/callbacks\n",
    "checkpoint_callback=ModelCheckpoint(\n",
    "                dirpath = 'checkpoint_dir',\n",
    "                save_top_k=1,\n",
    "                verbose=False,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "\n",
    "tube_logger = TestTubeLogger('checkpoint_dir', \n",
    "                            name='test_tube_logger')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 500,\n",
    "                     logger=tube_logger,\n",
    "                     progress_bar_refresh_rate=0,\n",
    "                     weights_summary=None,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     checkpoint_callback=checkpoint_callback,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                            patience=20)]) #the patience of 20 is mentioned in the DeepMicro paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qqq = LightningMAML(lightning.model, loss = torch.nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from learn2learn.data import MetaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLitFFNN(LitFFNN):\n",
    "    \n",
    "    def init_MAML_loss(self, ...):\n",
    "        lr=0.001\n",
    "        maml_lr=0.0005\n",
    "        #iterations=1000\n",
    "        ways=2\n",
    "        shots=1\n",
    "        # tps=16\n",
    "        tps = 5\n",
    "\n",
    "        fas=3\n",
    "        mu = 1\n",
    "\n",
    "        model = CompleteMHAPacking(proc_field, visit_field, emb_dim = 64,\n",
    "                                   hidden_size = 32, head_count = 2, dropout = .3)\n",
    "        model.to(device)\n",
    "\n",
    "        meta_model = l2l.algorithms.MAML(model, lr=maml_lr)\n",
    "        opt = optim.AdamW(meta_model.parameters(), lr=lr)\n",
    "\n",
    "        #loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "        # bias = np.log( ( np.array(ny) / np.max(np.array(ny)) ))\n",
    "        # linear = model.linear.state_dict()\n",
    "        # linear['bias'] = torch.tensor( bias )\n",
    "        # model.linear.load_state_dict(linear)\n",
    "\n",
    "\n",
    "        lr = 0.001\n",
    "        #  ClassBalancedFocalLoss(.95, 2), (.95, 5) also gave similarly poor results\n",
    "        #  (please tell me if im setting up this loss function incorrectly)\n",
    "\n",
    "\n",
    "        # ==> focus on Classbalanced\n",
    "        # loss_func = ClassBalancedFocalLoss(.95, ny, 2)\n",
    "        # loss_func.to(device)\n",
    "\n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "\n",
    "        #we have 4 tasks to meta-learn from\n",
    "        # most published approaches would not use the RA data in the metalearning step\n",
    "        #                         --> this doesn't make sense to me for this context\n",
    "        #              --> it makes more sense if we are stricly isolating how much carryover information we can get\n",
    "        #   --> to get best results i don't see why eliminating RA is necessary, but this is an easy thing to change in the future\n",
    "        num_tasks = 4\n",
    "\n",
    "        #determine number of datapoints to sample at each iteration\n",
    "        #samp_size = 100\n",
    "        iterations = 500\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            for _ in tqdm.tqdm( range(tps) ):\n",
    "                learner = self.model.clone( allow_unused=True )\n",
    "\n",
    "                #get data from a randomly selected task\n",
    "                        #for the 'experiment' currently only looking at source tasks for the metalearning\n",
    "                task = all_tasks[np.random.choice(4, 1)[0]]\n",
    "\n",
    "                for step in range(fas):\n",
    "                    # Compute validation loss\n",
    "                    learner.zero_grad()\n",
    "                    batch = next(task)\n",
    "                    preds = learner(batch)\n",
    "                    loss = loss_func(preds, to_categorical(batch.ird) )\n",
    "                    learner.adapt(loss)\n",
    "                    del batch, preds, loss\n",
    "\n",
    "                batch = get_batch(task)\n",
    "\n",
    "                source_loss = loss_func( learner(batch, device), to_categorical( batch.ird ) )\n",
    "\n",
    "                batch = next( all_tasks[0].__iter__() )\n",
    "                #get the loss with the fas-updated learner\n",
    "                preds = learner(batch, device)       \n",
    "\n",
    "                # Take the meta-learning step\n",
    "                opt.zero_grad()\n",
    "                val_error = loss_func(preds, to_categorical(batch.ird) ) #torch.as_tensor(batch.ird).long() )\n",
    "                #val_error.backward()\n",
    "\n",
    "                meta_pred_loss = val_error + mu * source_loss\n",
    "                meta_pred_loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                del batch, preds, val_error, learner\n",
    "\n",
    "            if iteration%5 == 0:\n",
    "                print(iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "dnabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
